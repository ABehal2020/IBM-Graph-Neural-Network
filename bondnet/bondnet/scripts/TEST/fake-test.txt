Namespace(batch_size=100, dataset_state_dict_filename='dataset_state_dict.pkl', dist_backend='nccl', dist_url='tcp://localhost:13456', distributed=0, embedding_size=24, epochs=1000, fc_activation='ReLU', fc_batch_norm=0, fc_dropout=0.0, fc_hidden_size=[64, 32], fc_num_layers=2, gated_activation='ReLU', gated_batch_norm=1, gated_dropout=0.0, gated_graph_norm=0, gated_hidden_size=[64, 64, 64], gated_num_fc_layers=1, gated_num_layers=3, gated_residual=1, gpu=None, lr=0.001, num_gpu=None, output_file='results.pkl', readout_type='bond', restore=0, start_epoch=0, weight_decay=0.0)


Start training at: 2023-01-13 14:53:14.218839
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 1., 3., 3., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [1., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [1., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [1., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [1., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [1., 0., 2., 3., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [1., 0., 2., 3., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [1., 1., 2., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [0., 0., 3., 4., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
         0., 0., 1., 0.],
        [1., 1., 2., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [1., 1., 2., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 1., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
{'feat': tensor([[0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [0., 0., 4., 4., 0., 0., 0., 3., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.],
        [1., 1., 2., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0.]])}
Labels are:  [{'value': tensor([-11.0100]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-4.8700]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([1.8300]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-5.4500]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-4.2100]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-6.2700]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([2.3400]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-3.9200]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([1.5800]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}, {'value': tensor([-4.6200]), 'num_bonds_in_molecule': tensor([1]), 'bond_index': tensor([0])}]
tensor([-11.0100,  -4.8700,   1.8300,  -5.4500,  -4.2100,  -6.2700,   2.3400,
         -3.9200,   1.5800,  -4.6200])
Trainset size: 8, valset size: 1: testset size: 1.
GatedGCNBond(
  (embedding): UnifySize(
    (linears): ModuleDict(
      (atom): Linear(in_features=22, out_features=24, bias=False)
      (bond): Linear(in_features=11, out_features=24, bias=False)
      (global): Linear(in_features=3, out_features=24, bias=False)
    )
  )
  (gated_layers): ModuleList(
    (0): GatedGCNConv(
      (activation): ReLU()
      (A): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (B): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (C): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (D): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (E): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (F): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (G): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (H): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (I): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=24, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (bn_node_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_u): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout): Identity()
    )
    (1): GatedGCNConv(
      (activation): ReLU()
      (A): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (B): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (C): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (D): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (E): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (F): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (G): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (H): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (I): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (bn_node_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_u): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout): Identity()
    )
    (2): GatedGCNConv(
      (activation): ReLU()
      (A): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (B): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (C): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (D): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (E): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (F): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (G): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (H): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (I): LinearN(
        (fc_layers): ModuleList(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Identity()
        )
      )
      (bn_node_h): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_e): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bn_node_u): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout): Identity()
    )
  )
  (fc_layers): ModuleList(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=1, bias=True)
  )
)


# Epoch     Loss         TrainAcc        ValAcc     Time (s)
    0   1.126536e+00   3.975071e+00   1.133072e+00   0.18
    1   1.096275e+00   3.879723e+00   1.066941e+00   0.06
    2   1.151839e+00   4.092958e+00   1.029104e+00   0.05
    3   1.088539e+00   3.887204e+00   1.004513e+00   0.06
    4   1.101628e+00   3.929808e+00   1.002502e+00   0.08
    5   1.119154e+00   3.862642e+00   9.861183e-01   0.06
    6   1.175587e+00   4.000075e+00   9.753659e-01   0.06
    7   1.163834e+00   3.725796e+00   9.781625e-01   0.05
    8   1.253871e+00   4.192634e+00   9.933712e-01   0.05
    9   1.082884e+00   3.807585e+00   1.004953e+00   0.05
   10   1.058746e+00   3.785575e+00   1.017939e+00   0.05
   11   1.155160e+00   3.907573e+00   1.016947e+00   0.05
   12   1.067557e+00   3.799040e+00   1.010906e+00   0.05
   13   1.104363e+00   4.009851e+00   9.913769e-01   0.05
   14   1.074395e+00   3.702925e+00   9.645488e-01   0.05
   15   1.067868e+00   3.884805e+00   9.153897e-01   0.05
   16   1.081751e+00   3.911257e+00   8.635811e-01   0.05
   17   1.009368e+00   3.771474e+00   8.075192e-01   0.05
   18   1.226547e+00   4.156421e+00   7.645979e-01   0.05
   19   1.035898e+00   3.845798e+00   7.233990e-01   0.05
   20   1.229465e+00   4.093208e+00   6.747410e-01   0.06
   21   1.027916e+00   3.614093e+00   6.820839e-01   0.05
   22   1.101885e+00   3.717791e+00   6.861961e-01   0.05
   23   1.042354e+00   3.684477e+00   6.874548e-01   0.05
   24   1.105587e+00   3.650804e+00   6.981776e-01   0.05
   25   1.017152e+00   3.707636e+00   7.378262e-01   0.05
   26   9.386212e-01   3.629804e+00   7.651526e-01   0.05
   27   1.067744e+00   3.735232e+00   7.842948e-01   0.07
   28   1.087646e+00   3.598644e+00   8.273228e-01   0.06
   29   1.076600e+00   3.699531e+00   8.700132e-01   0.06
   30   1.088959e+00   3.663188e+00   9.033608e-01   0.06
   31   1.172237e+00   4.018918e+00   9.368245e-01   0.06
   32   1.115952e+00   3.906806e+00   9.619116e-01   0.05
   33   9.149321e-01   3.661705e+00   9.792290e-01   0.05
   34   1.227927e+00   3.959750e+00   9.967556e-01   0.06
   35   1.067733e+00   3.856763e+00   1.016790e+00   0.07
   36   1.110922e+00   3.900829e+00   1.015390e+00   0.07
   37   1.108017e+00   3.869101e+00   1.020469e+00   0.06
   38   1.197566e+00   4.425449e+00   1.022333e+00   0.06
   39   1.099236e+00   3.907228e+00   1.006473e+00   0.05
   40   1.115628e+00   3.901292e+00   9.836870e-01   0.06
   41   1.138509e+00   3.995888e+00   9.446978e-01   0.06
   42   1.173389e+00   4.322815e+00   9.026423e-01   0.06
   43   1.106945e+00   3.788206e+00   8.975779e-01   0.06
   44   1.281816e+00   4.141512e+00   8.508352e-01   0.06
   45   1.127809e+00   3.955091e+00   8.022333e-01   0.06
   46   1.104283e+00   3.955183e+00   7.690441e-01   0.06
   47   1.117789e+00   3.890439e+00   7.469419e-01   0.06
   48   1.099858e+00   3.748456e+00   7.330736e-01   0.06
   49   8.744998e-01   3.177682e+00   6.666781e-01   0.06
   50   9.982082e-01   3.775366e+00   6.300826e-01   0.06
   51   1.159230e+00   3.998518e+00   5.735406e-01   0.06
   52   1.054066e+00   3.708906e+00   5.252144e-01   0.07
   53   1.180443e+00   4.025594e+00   4.498388e-01   0.07
   54   1.134733e+00   4.125783e+00   3.796456e-01   0.06
   55   1.139273e+00   3.972158e+00   3.173259e-01   0.06
   56   8.996734e-01   3.473068e+00   2.442041e-01   0.06
   57   1.075349e+00   3.933877e+00   1.703323e-01   0.07
   58   1.100361e+00   4.017100e+00   9.834414e-02   0.08
   59   1.086038e+00   3.663168e+00   7.034504e-02   0.08
   60   1.407639e+00   4.315676e+00   9.761164e-02   0.06
   61   1.149663e+00   3.846007e+00   1.170753e-01   0.06
   62   1.127389e+00   3.714427e+00   1.648740e-01   0.05
   63   1.529779e+00   4.752082e+00   1.899904e-01   0.15
   64   1.210077e+00   4.011334e+00   2.147504e-01   0.06
   65   1.188842e+00   4.009482e+00   2.335027e-01   0.07
   66   9.838606e-01   3.707337e+00   2.515279e-01   0.05
   67   1.053995e+00   3.682533e+00   2.563886e-01   0.06
   68   1.128062e+00   3.876390e+00   2.623787e-01   0.05
   69   8.992036e-01   3.481283e+00   2.561225e-01   0.06
   70   1.105440e+00   3.785103e+00   2.507252e-01   0.05
   71   1.131618e+00   3.768187e+00   2.604185e-01   0.05
   72   1.107221e+00   3.739746e+00   2.689313e-01   0.05
   73   1.116027e+00   3.932364e+00   2.709495e-01   0.05
   74   9.939353e-01   3.506542e+00   2.886438e-01   0.05
   75   1.281054e+00   4.007739e+00   3.203645e-01   0.05
   76   1.083332e+00   3.800231e+00   3.464621e-01   0.05
   77   1.240696e+00   4.159064e+00   3.797866e-01   0.05
   78   1.074764e+00   3.629478e+00   4.177493e-01   0.05
   79   1.145714e+00   4.029817e+00   4.496720e-01   0.05
   80   9.588099e-01   3.593952e+00   4.607365e-01   0.05
   81   1.146261e+00   3.709903e+00   4.663684e-01   0.05
   82   1.160166e+00   3.914623e+00   5.006501e-01   0.05
   83   1.224638e+00   3.994844e+00   5.549493e-01   0.05
   84   1.142618e+00   3.925619e+00   6.010048e-01   0.05
   85   1.149857e+00   3.884888e+00   6.617061e-01   0.05
   86   1.119890e+00   3.832730e+00   7.141141e-01   0.05
   87   1.143780e+00   3.974923e+00   7.667194e-01   0.06
   88   1.153590e+00   3.963660e+00   7.970403e-01   0.05
   89   1.009521e+00   3.522834e+00   8.277189e-01   0.05
   90   1.085037e+00   3.879776e+00   8.548036e-01   0.05
   91   1.087139e+00   3.732059e+00   8.823400e-01   0.05
   92   1.116390e+00   3.931163e+00   9.308536e-01   0.05
   93   1.346362e+00   4.246587e+00   9.815100e-01   0.05
   94   1.104442e+00   3.951918e+00   1.023304e+00   0.05
   95   9.587206e-01   3.442545e+00   1.065801e+00   0.05
   96   1.180819e+00   4.082491e+00   1.109997e+00   0.05
   97   1.189995e+00   3.908283e+00   1.150266e+00   0.05
   98   1.102568e+00   3.933508e+00   1.167789e+00   0.05
   99   1.109017e+00   3.999978e+00   1.169630e+00   0.05
  100   1.091291e+00   3.851115e+00   1.173700e+00   0.05
  101   1.099582e+00   3.855988e+00   1.175069e+00   0.05
  102   1.075880e+00   3.848172e+00   1.179705e+00   0.06
  103   1.136280e+00   4.077180e+00   1.169043e+00   0.05
  104   1.109420e+00   3.898819e+00   1.158684e+00   0.05
  105   1.082443e+00   3.901191e+00   1.155679e+00   0.06
  106   1.094144e+00   3.813818e+00   1.151736e+00   0.06
  107   1.100527e+00   3.957335e+00   1.157823e+00   0.06
  108   1.075548e+00   3.866728e+00   1.171817e+00   0.06
  109   1.121643e+00   3.883706e+00   1.187231e+00   0.06
Epoch 00111: reducing learning rate of group 0 to 4.0000e-04.
  110   1.157448e+00   3.976899e+00   1.179735e+00   0.06
  111   1.100615e+00   3.914229e+00   1.169200e+00   0.06
  112   1.178819e+00   4.136113e+00   1.156792e+00   0.05
  113   1.032174e+00   3.761567e+00   1.141283e+00   0.05
  114   1.043500e+00   3.795962e+00   1.125845e+00   0.06
  115   1.112420e+00   3.880312e+00   1.111419e+00   0.07
  116   1.062298e+00   3.890584e+00   1.095428e+00   0.07
  117   1.063743e+00   3.841457e+00   1.079966e+00   0.05
  118   1.094059e+00   3.842227e+00   1.064985e+00   0.05
  119   1.103007e+00   3.854636e+00   1.051205e+00   0.05
  120   1.094404e+00   3.958463e+00   1.038780e+00   0.06
  121   9.950571e-01   3.720482e+00   1.026028e+00   0.06
  122   1.110455e+00   4.003083e+00   1.010072e+00   0.07
  123   8.893166e-01   3.379168e+00   9.964653e-01   0.06
  124   1.239713e+00   3.855304e+00   9.828354e-01   0.06
  125   1.161416e+00   4.210576e+00   9.694934e-01   0.06
  126   1.094069e+00   3.934483e+00   9.561740e-01   0.06
  127   1.120113e+00   3.904318e+00   9.438109e-01   0.06
  128   1.084484e+00   3.708200e+00   9.333696e-01   0.07
  129   1.032459e+00   3.788440e+00   9.233776e-01   0.07
  130   9.470906e-01   3.650218e+00   9.137412e-01   0.06
  131   1.149520e+00   4.014489e+00   8.958021e-01   0.05
  132   1.002663e+00   3.775191e+00   8.787869e-01   0.06
  133   1.117645e+00   3.870077e+00   8.590232e-01   0.06
  134   1.134801e+00   3.763941e+00   8.421326e-01   0.07
  135   1.130132e+00   3.943314e+00   8.264876e-01   0.08
  136   8.575776e-01   3.209656e+00   8.159693e-01   0.06
  137   1.195989e+00   4.118384e+00   8.060415e-01   0.05
  138   1.115638e+00   4.116045e+00   7.970260e-01   0.06
  139   1.072095e+00   3.800551e+00   7.880449e-01   0.12
  140   1.173069e+00   3.983603e+00   7.789539e-01   0.08
  141   9.988904e-01   3.569785e+00   7.696022e-01   0.06
  142   1.069201e+00   3.918586e+00   7.606981e-01   0.06
  143   1.155487e+00   3.926258e+00   7.444921e-01   0.05
  144   1.126999e+00   4.122553e+00   7.303955e-01   0.05
  145   1.132603e+00   3.848006e+00   7.167485e-01   0.05
  146   9.636686e-01   3.705488e+00   7.037743e-01   0.06
  147   1.086382e+00   3.799410e+00   6.906834e-01   0.05
  148   1.090600e+00   4.063538e+00   6.782963e-01   0.05
  149   1.253873e+00   4.039832e+00   6.643456e-01   0.09
  150   1.170623e+00   3.931060e+00   6.496462e-01   0.05
  151   1.046941e+00   3.617549e+00   6.382837e-01   0.05
  152   1.069611e+00   3.788373e+00   6.278991e-01   0.05
  153   1.160562e+00   3.907645e+00   6.184902e-01   0.05
  154   9.477749e-01   3.735737e+00   6.095932e-01   0.05
  155   1.078049e+00   3.784601e+00   6.015351e-01   0.05
  156   1.123882e+00   3.892034e+00   5.857050e-01   0.06
  157   1.266353e+00   3.625215e+00   5.715234e-01   0.05
  158   1.168478e+00   3.892155e+00   5.601244e-01   0.05
  159   1.108481e+00   3.880567e+00   5.498466e-01   0.05
  160   1.243768e+00   3.694650e+00   5.398760e-01   0.05
Epoch 00162: reducing learning rate of group 0 to 1.6000e-04.
  161   1.043255e+00   3.789871e+00   5.302835e-01   0.05
  162   1.066839e+00   3.979407e+00   5.269605e-01   0.05
  163   1.128580e+00   3.861864e+00   5.266399e-01   0.05
  164   8.626059e-01   3.506576e+00   5.252956e-01   0.05
  165   1.065008e+00   3.775771e+00   5.239071e-01   0.05
  166   1.040417e+00   3.887048e+00   5.222492e-01   0.05
  167   1.104146e+00   3.879912e+00   5.253389e-01   0.05
  168   1.122859e+00   3.894459e+00   5.294358e-01   0.05
  169   8.409284e-01   3.470957e+00   5.320196e-01   0.05
  170   8.161877e-01   3.456753e+00   5.335869e-01   0.06
  171   1.226018e+00   3.966976e+00   5.359387e-01   0.07
  172   1.126014e+00   3.944459e+00   5.377783e-01   0.07
  173   9.232344e-01   3.614285e+00   5.392477e-01   0.07
  174   1.108071e+00   3.724532e+00   5.406629e-01   0.07
  175   8.885073e-01   3.515105e+00   5.412006e-01   0.08
  176   8.715453e-01   3.256800e+00   5.426199e-01   0.08
  177   1.038380e+00   3.789633e+00   5.481637e-01   0.09
  178   1.070861e+00   3.971238e+00   5.527702e-01   0.08
  179   8.763922e-01   3.225441e+00   5.578430e-01   0.09
  180   1.196489e+00   3.980908e+00   5.615258e-01   0.08
  181   1.115177e+00   3.895764e+00   5.647232e-01   0.05
  182   1.094438e+00   3.856167e+00   5.685318e-01   0.05
  183   1.135422e+00   3.944972e+00   5.716460e-01   0.08
  184   1.121660e+00   3.950999e+00   5.737556e-01   0.09
  185   7.472140e-01   3.231346e+00   5.742102e-01   0.09
  186   1.086955e+00   3.946406e+00   5.743845e-01   0.09
  187   9.653139e-01   3.545709e+00   5.744980e-01   0.07
  188   1.238444e+00   4.060659e+00   5.736311e-01   0.05
  189   1.165114e+00   3.835477e+00   5.734650e-01   0.05
  190   1.194795e+00   3.936730e+00   5.698375e-01   0.05
  191   1.121849e+00   3.863709e+00   5.692957e-01   0.05
  192   1.118768e+00   3.824319e+00   5.679726e-01   0.05
  193   1.285611e+00   4.059269e+00   5.649698e-01   0.05
  194   1.014918e+00   3.603729e+00   5.629382e-01   0.05
  195   1.089025e+00   3.850156e+00   5.611051e-01   0.05
  196   1.126809e+00   3.908178e+00   5.593044e-01   0.05
  197   1.058583e+00   3.707564e+00   5.568414e-01   0.05
  198   1.056368e+00   3.899805e+00   5.545043e-01   0.05
  199   1.119026e+00   3.907310e+00   5.520080e-01   0.05
  200   1.104685e+00   3.899518e+00   5.495552e-01   0.06
  201   8.524824e-01   3.618883e+00   5.463032e-01   0.07
  202   1.001664e+00   3.767555e+00   5.432881e-01   0.08
  203   1.102594e+00   3.776554e+00   5.372698e-01   0.07
  204   1.063188e+00   3.719438e+00   5.325387e-01   0.07
  205   8.309290e-01   3.157590e+00   5.293277e-01   0.05
  206   1.124112e+00   3.909252e+00   5.258896e-01   0.05
  207   8.226902e-01   3.161428e+00   5.236911e-01   0.10
  208   1.120988e+00   3.920029e+00   5.216414e-01   0.05

#TestAcc: 3.109848e-01 


Finish training at: 2023-01-13 14:53:27.114035
